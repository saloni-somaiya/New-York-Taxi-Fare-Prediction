{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4578d48b219735043a4d2102119fb307d2fc83f",
    "colab_type": "text",
    "id": "xpXpEW8hOlSr"
   },
   "source": [
    "# New York City Taxi Fare Prediction Playground Competition \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This is a playground competition on New York City Taxi Fare Prediction with a dataset of 44M rows. For better analysis and saving up the memory I have used limited data which are important for prediction. Here I am supposed to predict the taxi fare amount (inclusive of tolls) given the pickup and dropoff locations. I can get an estimation on just the distance between the two points and other features like datetime and passenger count. I have derived absolute travel vector features in both latitude and longitude co-ordinates for setting up the training data. For data cleaning purpose I dropped the NaN values and the features which show less correlation with the predicting feature using corrplot and heatmap. I removed the negative taxi fares, passenger count more than 6 or 0 and the longitude/latitude values which are out of range. A scatter plot of the trained absolute values is plotted to check the outliers in them and hence I removed them. I plotted the training data points on a map for visualization purpose. For better data preprocessing, I calculated the manhattan and euclidean distance and converted them into kilometer and the minimum and maximum distance values. I also notice that the longer the distance the more the fare and the taxi fare to airports is more especially during the night time. So I divided the time into morning, afternoon, evening and late night and also weekend or not. This will give me more insight in the analysis of the taxi fare. To make the predictions better I  have performed linear regression, random forest regression , neural network and gradient boosting for parameter tuning using tensorflow. These methods helps to reduce the Root Mean Square Error value and give accurate results for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "wjj-jDMROlSt",
    "outputId": "550f6d43-7444-4507-d04c-6a0524cb94b5"
   },
   "outputs": [],
   "source": [
    "# Initial Python environment setup...\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # CSV file I/O (e.g. pd.read_csv)\n",
    "import os # reading the input files we have access to\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import normalize,MinMaxScaler\n",
    "%matplotlib inline\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "#imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import layers\n",
    "\n",
    "# print(os.listdir('./gdrive/My Drive/'))\n",
    "training=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb969a26e52931bcaced3cbb7a36d8d8b1b04556",
    "colab_type": "text",
    "id": "IdY0Y1jROlSy"
   },
   "source": [
    "### Setup training data\n",
    "First let's read in our training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "649c3NDQOlSz"
   },
   "outputs": [],
   "source": [
    "if training:\n",
    "     train_df =  pd.read_csv('./new-york-city-taxi-fare-prediction/train.csv', nrows = 15_000_000)\n",
    "else:\n",
    "    train_df = pd.read_csv('./new-york-city-taxi-fare-prediction/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "25df18156ed90f583efdbbc028c58a9d2bdfdc7b",
    "colab_type": "text",
    "id": "MsWkygqEOlS2"
   },
   "source": [
    "Let's create two new features in our training set representing the \"travel vector\" between the start and end points of the taxi ride, in both longitude and latitude coordinates.  We'll take the absolute value since we're only interested in distance traveled. Use a helper function since we'll want to do the same thing for the test set later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_uuid": "59f0595db44dd60044cfd0404824651a7c2bee87",
    "colab": {},
    "colab_type": "code",
    "id": "l3yqlg3lOlS3"
   },
   "outputs": [],
   "source": [
    "# Given a dataframe, add two new features 'abs_diff_longitude' and\n",
    "# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1dbc7610bd467f1dfaf9042b5ec638eb2014aaf",
    "colab_type": "text",
    "id": "hgc4Ni6EOlS6"
   },
   "source": [
    "### Data Operations\n",
    "First let's see if there are any `NaN`s in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "_uuid": "e808c7e75338b45ca30f9f261dfbc90845700624",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-ghv8oSAOlS7",
    "outputId": "8bbd192c-bf19-4018-83ae-09fa0681130e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key                    0\n",
      "fare_amount            0\n",
      "pickup_datetime        0\n",
      "pickup_longitude       0\n",
      "pickup_latitude        0\n",
      "dropoff_longitude     99\n",
      "dropoff_latitude      99\n",
      "passenger_count        0\n",
      "abs_diff_longitude    99\n",
      "abs_diff_latitude     99\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "29bc86f2fa8baa37f0c4eb4300f77a8cb69f12aa",
    "colab_type": "text",
    "id": "XrGOEbvNOlS_"
   },
   "source": [
    "There are a small amount, so let's remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "_uuid": "9d8f28e24f3d4ca55ad93692329680774c341376",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nQtO4rOjOlTA",
    "outputId": "eaffb8d6-30f9-4efc-8ddc-3204ed4de227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 15000000\n",
      "New size: 14999901\n"
     ]
    }
   ],
   "source": [
    "print('Old size: %d' % len(train_df))\n",
    "if training:\n",
    "    train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a045ef14c636ec726a5e8c349ca7e5fbb3a87c1",
    "colab_type": "text",
    "id": "jglhLV2rOlTD"
   },
   "source": [
    "Now let's quickly plot a subset of our travel vector features to see its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "_uuid": "97d0aa1deab1c6cf0c97a4a3a12ba7007aada6c5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "kqwKLCwwOlTD",
    "outputId": "9015e665-a300-48e8-f894-e83b2dd51a2f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot= train_df.iloc[:20000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22277d77f75e3177a5acaec9b820e0de6e869663",
    "colab_type": "text",
    "id": "-K9YqX39OlTH"
   },
   "source": [
    "We expect most of these values to be very small (likely between 0 and 1) since it should all be differences between GPS coordinates within one city.  For reference, one degree of latitude is about 69 miles.  However, we can see the dataset has extreme values which do not make sense.  Let's remove those values from our training set. Based on the scatterplot, it looks like we can safely exclude values above 5 (though remember the scatterplot is only showing the first 2000 rows...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "_uuid": "9703895e6c7e67b32c504f843b5ef19be2023964",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-wZ92lRjOlTH",
    "outputId": "4e557b78-ede4-4ac7-9df9-d0a831bdfac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 14999901\n",
      "New size: 14968927\n"
     ]
    }
   ],
   "source": [
    "print('Old size: %d' % len(train_df))\n",
    "if training:\n",
    "    train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "m4p6yOLWOlTK",
    "outputId": "2a6dd9bb-528d-445b-d71b-4a9c7ebd0f7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>abs_diff_longitude</th>\n",
       "      <th>abs_diff_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-06-15 17:26:21.0000001</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2009-06-15 17:26:21 UTC</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.009041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05 16:52:16.0000002</td>\n",
       "      <td>16.9</td>\n",
       "      <td>2010-01-05 16:52:16 UTC</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>0.070701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-08-18 00:35:00.00000049</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2011-08-18 00:35:00 UTC</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.010708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-04-21 04:30:42.0000001</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2012-04-21 04:30:42 UTC</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.024949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-03-09 07:51:00.000000135</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2010-03-09 07:51:00 UTC</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.015754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             key  fare_amount          pickup_datetime  \\\n",
       "0    2009-06-15 17:26:21.0000001          4.5  2009-06-15 17:26:21 UTC   \n",
       "1    2010-01-05 16:52:16.0000002         16.9  2010-01-05 16:52:16 UTC   \n",
       "2   2011-08-18 00:35:00.00000049          5.7  2011-08-18 00:35:00 UTC   \n",
       "3    2012-04-21 04:30:42.0000001          7.7  2012-04-21 04:30:42 UTC   \n",
       "4  2010-03-09 07:51:00.000000135          5.3  2010-03-09 07:51:00 UTC   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.844311        40.721319         -73.841610         40.712278   \n",
       "1        -74.016048        40.711303         -73.979268         40.782004   \n",
       "2        -73.982738        40.761270         -73.991242         40.750562   \n",
       "3        -73.987130        40.733143         -73.991567         40.758092   \n",
       "4        -73.968095        40.768008         -73.956655         40.783762   \n",
       "\n",
       "   passenger_count  abs_diff_longitude  abs_diff_latitude  \n",
       "0                1            0.002701           0.009041  \n",
       "1                1            0.036780           0.070701  \n",
       "2                2            0.008504           0.010708  \n",
       "3                1            0.004437           0.024949  \n",
       "4                1            0.011440           0.015754  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2151480a168d291bc2f4fd014fdac4ab7b5f6560",
    "colab_type": "text",
    "id": "9DJH2F2JOlTL"
   },
   "source": [
    "\n",
    "### Train our model\n",
    " Considering only the selected values in the training data:\n",
    " 1. Passenger count which is less than 6 makes sense\n",
    " 2. Positive fare amount values\n",
    " 3. Fare amount less  than 250\n",
    " 4. Airport co-ordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xdZrgGwrOlTM",
    "outputId": "cf2519a6-4978-4f35-c3f0-c9890c54a5bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manushpatel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "if training:\n",
    "    train_df = train_df[train_df[\"passenger_count\"] < 6]\n",
    "    train_df_1 = train_df[train_df[\"fare_amount\"] > 0]\n",
    "\n",
    "    train_df_1 = train_df_1[train_df[\"fare_amount\"] < 250]\n",
    "    jfk = (-73.7822222222, 40.6441666667) #JFK Airport\n",
    "    ewr = (-74.175, 40.69) # Newark Liberty International Airport\n",
    "    lgr = (-73.87, 40.77) # LaGuardia Airport\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data = train_df_1[(train_df_1.abs_diff_longitude < 3.0) & (train_df_1.abs_diff_latitude < 3.0)]\n",
    "    data = data[(data.pickup_longitude >= -74.3) & (data.pickup_longitude <= -72.9)]  # nyc coordinates\n",
    "    data = data[(data.dropoff_longitude >= -74.3) & (data.dropoff_longitude <= -72.9)]\n",
    "    data = data[(data.pickup_latitude >= 40.5) & (data.pickup_latitude <= 41.8)]\n",
    "    data = data[(data.dropoff_latitude >= 40.5) & (data.dropoff_latitude <= 41.8)]\n",
    "    train_df_1 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqxJ81NqOlTO"
   },
   "outputs": [],
   "source": [
    "train_df_1 = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146522760"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "_uuid": "85abbb09a27d2e1e2a15b261264b3c7cbdde39e4",
    "colab": {},
    "colab_type": "code",
    "id": "LbVINcS7OlTQ"
   },
   "outputs": [],
   "source": [
    "train_df_abs_diffs = train_df_1\n",
    "    \n",
    "\n",
    "x_data = train_df_abs_diffs[[\"abs_diff_longitude\",\"abs_diff_latitude\",\"passenger_count\",\"pickup_datetime\",\n",
    "                             'pickup_latitude',\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\"]]\n",
    "if training:\n",
    "    y_data = train_df_abs_diffs[\"fare_amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Implementing various functions to calculate the distance between the pickup latitude, longitude points and dropff latitude, longitude points. \n",
    "Also , we have encoding for time where we have splitted the 24 hour clock into 4 parts i.e morning, afternoon,  evening and late night as at each time the fares are different. Year,month and day are also considered as weekend rates are more especially while for the airport locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCuKOXPfOlTR"
   },
   "outputs": [],
   "source": [
    "#lambdas\n",
    "\n",
    "#print(data.describe())\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = 0.017453292519943295 # Pi/180\n",
    "    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a)) # 2*R*asin...\n",
    "  \n",
    "def calc_sin_square(x):\n",
    "    return math.sin(math.radians(x/2)) ** 2\n",
    "\n",
    "def calc_c(x):\n",
    "    return math.atan2(math.sqrt(x),math.sqrt(1-x))\n",
    "\n",
    "def act_distance(x):\n",
    "    return 6371 * x\n",
    "\n",
    "def label_time(x):\n",
    "    return x.split(\" \")[1][:2]\n",
    "\n",
    "def encode_time(x):\n",
    "    a = int(x)\n",
    "    if a >=0 and a<=6:\n",
    "        return 1\n",
    "    elif a>6 and a <=12:\n",
    "        return 2\n",
    "    elif a>12 and a<=18:\n",
    "        return 3\n",
    "    elif a>18 and a<=24:\n",
    "        return 4\n",
    "def late_night_calc(x):\n",
    "    a = int(x.split(\" \")[1][:2])\n",
    "    if a >= 22 or a<=6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def weekend_or_not(x):\n",
    "    date = x.split(\" \")[0]\n",
    "    weekno = datetime.strptime(date,\"%Y-%m-%d\").weekday()\n",
    "    if weekno < 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "def year(x):\n",
    "    date = x.split(\" \")[0]\n",
    "    year = datetime.strptime(date,\"%Y-%m-%d\").year\n",
    "    return year\n",
    "def month(x):\n",
    "    date = x.split(\" \")[0]\n",
    "    month = datetime.strptime(date,\"%Y-%m-%d\").month\n",
    "    return month\n",
    "def date(x):\n",
    "    date = x.split(\" \")[0]\n",
    "    month = datetime.strptime(date,\"%Y-%m-%d\").day\n",
    "    return month\n",
    "    \n",
    "def exp_decay(epoch):\n",
    "    return epoch ** 0.095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kE8HVX2GOlTS"
   },
   "source": [
    "### Distance conversion lat,long to km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "FvDuJcIEOlTT",
    "outputId": "baf07d02-1e7b-4bd9-d1a6-b9530858d802"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manushpatel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x_data[\"a_lat\"] = x_data[\"abs_diff_latitude\"].apply(calc_sin_square)\n",
    "x_data[\"a_long\"] = x_data[\"abs_diff_longitude\"].apply(calc_sin_square)\n",
    "x_data[\"c_lat\"] = x_data[\"a_lat\"].apply(calc_c)\n",
    "x_data[\"c_long\"] = x_data[\"a_long\"].apply(calc_c)\n",
    "x_data[\"lat_dist\"]  = x_data[\"c_lat\"].apply(act_distance)\n",
    "x_data[\"long_dist\"] = x_data[\"c_long\"].apply(act_distance)\n",
    "x_data[\"distance_km\"] = x_data[\"lat_dist\"] + x_data[\"long_dist\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more features in the model like pickup and dropoff at popular places of New York where. passengers travels the most for better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhThc9iF6l-S"
   },
   "outputs": [],
   "source": [
    "nyc = (-74.0063889, 40.7141667)\n",
    " # county\n",
    "Nassau = (-73.5594, 40.6546)\n",
    "Suffolk = (-72.6151, 40.9849)\n",
    "Westchester = (-73.7949, 41.1220)\n",
    "Rockland = (-73.9830, 41.1489)\n",
    "Dutchess = (-73.7478, 41.7784)\n",
    "Orange = (-74.3118, 41.3912)\n",
    "Putnam = (-73.7949, 41.4351) \n",
    "x_data['distance_to_center'] = distance(nyc[1], nyc[0],x_data.dropoff_latitude, x_data.dropoff_longitude)\n",
    "x_data[\"pickup_nyc\"] = distance(nyc[1], nyc[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "x_data[\"pickup_jfk\"] = distance(jfk[1], jfk[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "x_data[\"pickup_ewr\"] = distance(ewr[1], ewr[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "x_data[\"pickup_lgr\"] = distance(lgr[1], lgr[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "x_data[\"dropoff_nass\"] = distance(Nassau[1], Nassau[0],x_data.dropoff_latitude, x_data.dropoff_longitude)\n",
    "x_data[\"dropoff_suff\"] = distance(Suffolk[1], Suffolk[0],x_data.dropoff_latitude, x_data.dropoff_longitude)\n",
    "x_data[\"dropoff_west\"] = distance(Westchester[1], Westchester[0],x_data.dropoff_latitude, x_data.dropoff_longitude)\n",
    "x_data[\"dropoff_rock\"] = distance(Rockland[1], Rockland[0],x_data.dropoff_latitude, x_data.dropoff_longitude)\n",
    "x_data[\"dropoff_dutc\"] = distance(Dutchess[1], Dutchess[0],x_data.dropoff_latitude, x_data.dropoff_longitude)\n",
    "x_data[\"dropoff_oran\"] = distance(Orange[1], Orange[0],x_data.dropoff_latitude, x_data.dropoff_longitude)\n",
    "x_data[\"dropoff_putn\"] = distance(Putnam[1], Putnam[0],x_data.dropoff_latitude, x_data.dropoff_longitude)\n",
    "x_data[\"pickup_nass\"] = distance(Nassau[1], Nassau[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "x_data[\"pickup_suff\"] = distance(Suffolk[1], Suffolk[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "x_data[\"pickup_west\"] = distance(Westchester[1], Westchester[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "x_data[\"pickup_rock\"] = distance(Rockland[1], Rockland[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "x_data[\"pickup_dutc\"] = distance(Dutchess[1], Dutchess[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "x_data[\"pickup_oran\"] = distance(Orange[1], Orange[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "x_data[\"pickup_putn\"] = distance(Putnam[1], Putnam[0],x_data.pickup_latitude, x_data.pickup_longitude)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DkVX0F6OlTU"
   },
   "source": [
    "### Time conversion from string to label 1,2,3,4 & weekend or not & late night or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13rp85LyOlTU"
   },
   "outputs": [],
   "source": [
    "x_data[\"time_label\"] = x_data[\"pickup_datetime\"].apply(label_time)\n",
    "x_data[\"time\"] = x_data[\"time_label\"].apply(encode_time)\n",
    "\n",
    "\n",
    "\n",
    "x_data[\"late_night\"] = x_data[\"pickup_datetime\"].apply(late_night_calc)\n",
    "x_data[\"weekend\"] = x_data[\"pickup_datetime\"].apply(weekend_or_not)\n",
    "x_data[\"year\"] = x_data[\"pickup_datetime\"].apply(year)\n",
    "x_data[\"date\"] = x_data[\"pickup_datetime\"].apply(date)\n",
    "x_data[\"month\"] = x_data[\"pickup_datetime\"].apply(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTNqULWfOlTV"
   },
   "outputs": [],
   "source": [
    "x_data = x_data.drop([\"passenger_count\",\"a_lat\",\"dropoff_latitude\",\"pickup_latitude\",\"pickup_longitude\",\"pickup_latitude\",\n",
    "                      \"a_long\",\"c_lat\",\"c_long\",\"lat_dist\",\"time_label\",\"pickup_datetime\",\"long_dist\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8m5J2J1MOlTX",
    "outputId": "3c229fe1-9ad4-4886-b761-282029288f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2930456 11721820 2930456 11721820\n"
     ]
    }
   ],
   "source": [
    "if training:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.20, random_state=42)\n",
    "    print(len(X_test),len(X_train),len(y_test),len(y_train))\n",
    "else:\n",
    "    X_test = x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abs_diff_longitude    float64\n",
       "abs_diff_latitude     float64\n",
       "dropoff_longitude     float64\n",
       "distance_km           float64\n",
       "distance_to_center    float64\n",
       "pickup_nyc            float64\n",
       "pickup_jfk            float64\n",
       "pickup_ewr            float64\n",
       "pickup_lgr            float64\n",
       "dropoff_nass          float64\n",
       "dropoff_suff          float64\n",
       "dropoff_west          float64\n",
       "dropoff_rock          float64\n",
       "dropoff_dutc          float64\n",
       "dropoff_oran          float64\n",
       "dropoff_putn          float64\n",
       "pickup_nass           float64\n",
       "pickup_suff           float64\n",
       "pickup_west           float64\n",
       "pickup_rock           float64\n",
       "pickup_dutc           float64\n",
       "pickup_oran           float64\n",
       "pickup_putn           float64\n",
       "time                    int64\n",
       "late_night              int64\n",
       "weekend                 int64\n",
       "year                    int64\n",
       "date                    int64\n",
       "month                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqh-Mpr0OlTY"
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ULt8zqZOlTZ"
   },
   "outputs": [],
   "source": [
    "if training:\n",
    "    regr = linear_model.LinearRegression(normalize = True)\n",
    "    regr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2xPTHWqOlTa"
   },
   "outputs": [],
   "source": [
    "if training:\n",
    "    y_pred = regr.predict(X_test)\n",
    "else:\n",
    "    y_pred = regr.predict(x_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "eTicBNv6OlTb",
    "outputId": "cbae8ccb-2a89-4581-ae1a-7f68aca0b7e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [-9.17908658e+12 -9.17908658e+12  4.07118551e-03  1.65099018e+11\n",
      "  7.95934089e-01  1.20764990e+00 -3.52910961e-01 -2.68049272e+00\n",
      "  3.64837284e-01  1.05481853e-01 -1.52754422e+00 -1.36798256e+00\n",
      "  7.95920090e+00  5.01349192e+00 -6.77126485e+00 -4.50900521e+00\n",
      " -5.10982766e-01  2.75560247e+00 -4.66039531e+00 -4.44593340e+00\n",
      " -8.33648329e+00  9.29042365e+00  7.67301406e+00 -2.55934112e-02\n",
      " -2.68830941e-01 -2.24058928e-01  5.46332135e-01  1.82988227e-03\n",
      "  8.37812088e-02]\n",
      "Root Mean squared error: 6.15\n",
      "Variance score: 0.61\n"
     ]
    }
   ],
   "source": [
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Root Mean squared error: %.2f\"\n",
    "      % math.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_NeyRpdFOlTd"
   },
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ensemble method we train multiple models using the same learning algorithm. It helps to minimize noise, bias variance. A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of a data set to improve the predictive accuracy and control over fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beOP4zCbOlTd"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FPmqtazOlTe"
   },
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=200,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DRgGeCdOlTg"
   },
   "outputs": [],
   "source": [
    "regr.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "av-LhGeWOlTi"
   },
   "outputs": [],
   "source": [
    "error = abs(y_pred - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdLMVxLXOlTj",
    "outputId": "ac1ea659-12f1-4350-db7c-e930b67dae93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.83531437371066"
      ]
     },
     "execution_count": 192,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(100 * (error / y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUJUbvw5OlTk",
    "outputId": "60326c98-4a9d-4121-f56f-9913a8ce68ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.642046290748097"
      ]
     },
     "execution_count": 195,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanSquaredError=mean_squared_error(y_test,y_pred)\n",
    "math.sqrt(meanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZt8axDOOlTl"
   },
   "source": [
    "### Neural network Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the input layer by adjusting and scaling the activations. Batch normalization reduces the amount by what the hidden unit values shift around (covariance shift). It reduces overfitting because it has a slight regularization effects. Similar to dropout, it adds some noise to each hidden layer’s activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y25w9KvkOlTl"
   },
   "outputs": [],
   "source": [
    "#build our model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "model.add(Dense(256))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "model.add(Dense(128))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "model.add(Dense(64))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "model.add(Dense(32))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yJ-46_lOlTm",
    "outputId": "8ca4f33e-9aa2-448d-c8fa-6db4770835cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7814316/7814316 [==============================] - 215s 27us/step - loss: 34.8245\n",
      "Epoch 2/50\n",
      "7814316/7814316 [==============================] - 214s 27us/step - loss: 24.6109\n",
      "Epoch 3/50\n",
      "7814316/7814316 [==============================] - 911s 117us/step - loss: 24.4205\n",
      "Epoch 4/50\n",
      "7814316/7814316 [==============================] - 916s 117us/step - loss: 24.3112\n",
      "Epoch 5/50\n",
      "7814316/7814316 [==============================] - 228s 29us/step - loss: 24.2535\n",
      "Epoch 6/50\n",
      "7814316/7814316 [==============================] - 226s 29us/step - loss: 24.1975\n",
      "Epoch 7/50\n",
      "7814316/7814316 [==============================] - 10778s 1ms/step - loss: 24.1302\n",
      "Epoch 8/50\n",
      "7814316/7814316 [==============================] - 3564s 456us/step - loss: 24.1023\n",
      "Epoch 9/50\n",
      "7814316/7814316 [==============================] - 3401s 435us/step - loss: 24.1017\n",
      "Epoch 10/50\n",
      "7814316/7814316 [==============================] - 3132s 401us/step - loss: 24.0767\n",
      "Epoch 11/50\n",
      "7814316/7814316 [==============================] - 8253s 1ms/step - loss: 24.0614\n",
      "Epoch 12/50\n",
      "7814316/7814316 [==============================] - 12825s 2ms/step - loss: 24.0444\n",
      "Epoch 13/50\n",
      "7814316/7814316 [==============================] - 231s 30us/step - loss: 24.0212\n",
      "Epoch 14/50\n",
      "7814316/7814316 [==============================] - 232s 30us/step - loss: 23.9959\n",
      "Epoch 15/50\n",
      "7814316/7814316 [==============================] - 233s 30us/step - loss: 24.0091\n",
      "Epoch 16/50\n",
      "7814316/7814316 [==============================] - 232s 30us/step - loss: 23.9778\n",
      "Epoch 17/50\n",
      "7814316/7814316 [==============================] - 234s 30us/step - loss: 23.9768\n",
      "Epoch 18/50\n",
      "7814316/7814316 [==============================] - 248s 32us/step - loss: 23.9655\n",
      "Epoch 19/50\n",
      "3751936/7814316 [=============>................] - ETA: 2:15 - loss: 23.6699"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train.values,\n",
    "    y_train.values,\n",
    "    batch_size= 4096,\n",
    "    epochs=50,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFkl5pOzOlTn",
    "outputId": "c509ee25-6eda-4306-d9c6-945034e897c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3223406/3223406 [==============================] - 36s 11us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.9082809484097"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(model.evaluate(x=X_test,y=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSn7JLa7OlTp"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a method of converting weak learners into strong learners. In boosting, each new tree is a fit on a modified version of the original data set. Gradient Boosting trains many models in a gradual, additive and sequential manner which helps in increasing the accuracy of the model and apparently this turns out to be the best model out of all as the RMSE value is 3.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'boosting_type':'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'nthread': 4,\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': -1,\n",
    "        'subsample': 0.8,\n",
    "        'bagging_fraction' : 1,\n",
    "        'max_bin' : 5000 ,\n",
    "        'bagging_freq': 20,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'metric': 'rmse',\n",
    "        'min_split_gain': 0.5,\n",
    "        'min_child_weight': 1,\n",
    "        'min_child_samples': 10,\n",
    "        'scale_pos_weight':1,\n",
    "        'zero_as_missing': True,\n",
    "        'seed':0,\n",
    "        'num_rounds':50000\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matrix_train = lgbm.Dataset(X_train, y_train)\n",
    "matrix_test = lgbm.Dataset(X_test, y_test)\n",
    "model=lgbm.train(params=params,\n",
    "                train_set=matrix_train,\n",
    "                num_boost_round=100000, \n",
    "                early_stopping_rounds=7000,\n",
    "                verbose_eval=100,\n",
    "                valid_sets=matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([433, 521, 453, 478, 752,  46, 113,  54])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a70ed21b43d720282bbae70e934b1188be2bc382",
    "colab_type": "text",
    "id": "ntnGNtHPOlTq"
   },
   "source": [
    "### Make predictions on the test set\n",
    "Now let's load up our test inputs and predict the `fare_amount`s for them using our learned weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_y = model.predict(X_test, num_iteration = model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddba4a856ff617411a641dfdf7635e47f969dff8",
    "colab": {},
    "colab_type": "code",
    "id": "HZbtOcbrOlTu",
    "outputId": "90ff2562-57f2-4c35-b227-69ffa03f37e5"
   },
   "outputs": [],
   "source": [
    "# Reuse the above helper functions to add our features and generate the input matrix.\n",
    "add_travel_vector_features(test_df)\n",
    "# test_X = get_input_matrix(test_df)\n",
    "# Predict fare_amount on the test set using our model (w) trained on the training set.\n",
    "\n",
    "# Write the predictions to a CSV file which we can submit to the competition.\n",
    "submission = pd.DataFrame(\n",
    "    {'key': train_df.key, 'fare_amount': pred_test_y})\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "Out of all the models created, Gradient Boosting gave me the better RMSE value than linear regression model, random forest regression and neural network. This kernel could predict the taxi fare in New York City based on the pickup and dropff points. From this model we conclude that the RMSE score does not go below 3 which the other kernels could do. The best model in this competition has the score of around 1.5. The deep layers of the features could not be extracted up to that level which could improve the score of the model. But we also observe that there are no outliers and there is not over fitting of the data. The loss function value is also accurate enough to make the predictions. Data cleaning, pre-processing and feature engineering was done to improve the features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook my aim is to get very small RMSE value than other public kernels and perform better analysis to predict the taxi fare. This notebook is a framework for testing multiple models, selecting the best one and analysing the best model. This kernel mainly uses pandas, numpy, sklearn and statsmodels for the critical work. For data cleaning I have removed the NaN values and other unwanted features which did not help in prediction. For data pre-processing, distances were calculated from pick up and drop off locations, airports and the day/time at which the taxi was booked. For data exploration I have used heatmap and map to plot the pickup latitude and pickup longitude using various features. Feature extraction and engineering is really essential in this competition. I have implemented linear regression and fitted the model. For deep learning I have used tensorflow and keras to perform random forest regression and XGBoost on the training (80%) and testing data (20%). Apart from the features in the dataset, I have added few more features and trained them to improve the model. I have kept the difference between the co-ordinates small as they should lie within the city. I have implemented various functions (lamdas) to calculate the distances and convert the date/time data. Weights are learned to make the predictions on the test data. I have achieved the RMSE value of 3.81 which could have been improved by training the data more deeply and extracted the features which help in estimating the taxi fare more accurately. Due to memory issues I have not used the whole dataset for analysis. Using more data could give better insight to the model as lesser the RMSE more accurate the prediction will be. It is a challenge for us to clean the data well and remove the noisy data. Also, parameter tuning plays a major role and there are various other methods to implement it.\n",
    "\n",
    "Apart from what other kernels have done in the competition, I have used more features approx. 27 for training the data.  We tried various configurations of neural network and performed batch normalization for deep learning on 512 dense layers. Also I have tried implementing ensemble modeling and light gradient boosting  to achieve the best score and minimum loss. My RMSE value is 3.90 and the Kaggle competition score is 3.817 which is not a good score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution Statement\n",
    "\n",
    " By own : 30%\n",
    " \n",
    " By external source : 70%\n",
    "\n",
    "Data Cleaning and pre-processing was done using various methods.\n",
    "\n",
    "Feature selection and extraction was done. \n",
    "Data exploration using plots was done.\n",
    "\n",
    "Training the data and predicting them was done using deep learning.\n",
    "\n",
    " XGBM was performed on test values which ensemble machine learning method and then final results were produced.\n",
    " \n",
    " Referred the kernels to get better insight about the techniques to be used in order to beat their score on the leaderboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "[1] https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/kernels  (For assignment reference)\n",
    "\n",
    "[2] https://scikit-learn.org/stable/modules/preprocessing.html  (For preprocessing the data)\n",
    "\n",
    "[3] https://plot.ly/python/maps/  (For data exploration)\n",
    "\n",
    "[4] https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e  ( For Parameter tuning)\n",
    "\n",
    "[5] https://testlightgbm.readthedocs.io/en/latest/Parameters.html    (Light Gradient boosting)\n",
    "\n",
    "[6]  https://www.tensorflow.org/guide/keras   (Guide for tensorflow and keras)\n",
    "\n",
    "[7] https://keras.io/callbacks/      (For learning rate scheduler)\n",
    "\n",
    "[8] https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html  (Random Forest Regression)\n",
    "\n",
    "[9] https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c   (Neural Network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## License\n",
    "\n",
    "Copyright 2019 SALONI SOMAIYA\n",
    "    \n",
    "    \n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of nyc-taxi-fare-starter-kernel-simple-linear-model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
